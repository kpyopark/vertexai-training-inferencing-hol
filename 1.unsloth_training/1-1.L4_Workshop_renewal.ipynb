{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07933467-0daf-4bcc-bb58-2c49012cb053",
   "metadata": {},
   "source": [
    "# Unsloth를 이용한 Gemma Model Fine Tuning\n",
    "\n",
    "Unsloth를 이용하면 메모리 Foot Print를 최소화하면서, PEFT를 효율적으로 진행할 수 있습니다. \n",
    "\n",
    "패키지 설치, 데이터셋 구성, 트레이닝 이후 테스트까지 진행해보도록 하겠습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uD-9Dvs-OYx-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD-9Dvs-OYx-",
    "outputId": "1aa936f9-7c57-4c0a-ab9b-0de6d5d30bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WRbM14IOLVn8",
   "metadata": {
    "id": "WRbM14IOLVn8"
   },
   "source": [
    "이 셀에서는 필요한 Python 패키지를 설치합니다. `torch`, `torchvision`, `torchaudio`, `xformers`는 PyTorch 생태계의 핵심 라이브러리이며, 딥러닝 모델을 구축하고 학습하는 데 사용됩니다. 특히, `xformers`는 메모리 효율적이고 빠른 연산을 위한 최적화된 연산자를 제공합니다. `unsloth`는 대규모 언어 모델을 미세 조정하는 과정을 단순화하고 최적화하는 데 초점을 맞춘 라이브러리입니다. 이 라이브러리는 모델 로딩, 미세 조정, 추론 과정을 간편하게 만들어줍니다. 마지막으로, `transformers`는 Hugging Face에서 개발한 자연어 처리 모델 라이브러리입니다. 이 셀에서는 `transformers` 라이브러리를 특정 버전으로 강제 설치하여 호환성 문제를 방지하고 있습니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **PyTorch 생태계:** 딥러닝 모델 개발에 필수적인 라이브러리들을 설치합니다.\n",
    "*   **`unsloth` 라이브러리:** 대규모 언어 모델의 미세 조정을 위한 핵심 라이브러리로, 설치함으로써 미세 조정 과정을 단순화합니다.\n",
    "*   **`transformers` 라이브러리:** 특정 버전으로 고정하여 호환성 문제를 방지합니다. 이는 특정 버전의 라이브러리에 최적화된 코드를 사용하거나, 라이브러리 간의 충돌을 방지하기 위함입니다.\n",
    "*   **GPU 가속:** CUDA 12.1 버전에 맞춰 PyTorch 및 관련 라이브러리를 설치하여 GPU 가속을 활용합니다. 이는 모델 학습 및 추론 속도를 향상시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vOcc74fvJrCacslv9BjJSkvi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "vOcc74fvJrCacslv9BjJSkvi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5c07beee-b8c7-4cd9-88e2-99ff6c2024a1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (780.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m177.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting xformers\n",
      "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m165.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m195.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m178.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers, torchvision, torchaudio\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0 xformers-0.0.29.post1\n",
      "Collecting unsloth==2025.1.1\n",
      "  Downloading unsloth-2025.1.1-py3-none-any.whl.metadata (60 kB)\n",
      "Collecting unsloth_zoo>=2025.1.1 (from unsloth==2025.1.1)\n",
      "  Downloading unsloth_zoo-2025.1.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (2.5.1+cu121)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (0.0.29.post1)\n",
      "Collecting bitsandbytes (from unsloth==2025.1.1)\n",
      "  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (3.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (24.2)\n",
      "Collecting tyro (from unsloth==2025.1.1)\n",
      "  Downloading tyro-0.9.13-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting transformers!=4.47.0,>=4.46.1 (from unsloth==2025.1.1)\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets>=2.16.0 (from unsloth==2025.1.1)\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth==2025.1.1)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (5.9.3)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (0.45.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (1.26.4)\n",
      "Collecting accelerate>=0.34.1 (from unsloth==2025.1.1)\n",
      "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth==2025.1.1)\n",
      "  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth==2025.1.1)\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (0.27.1)\n",
      "Collecting hf_transfer (from unsloth==2025.1.1)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.1->unsloth==2025.1.1) (6.0.2)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.1->unsloth==2025.1.1)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (15.0.2)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth==2025.1.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (2.32.3)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth==2025.1.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth==2025.1.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth==2025.1.1)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->unsloth==2025.1.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth==2025.1.1) (1.3.0)\n",
      "Collecting regex!=2019.12.17 (from transformers!=4.47.0,>=4.46.1->unsloth==2025.1.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers!=4.47.0,>=4.46.1->unsloth==2025.1.1)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.1.1) (13.9.4)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.1.1->unsloth==2025.1.1)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from unsloth_zoo>=2025.1.1->unsloth==2025.1.1) (11.0.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth==2025.1.1) (0.16)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth==2025.1.1)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth==2025.1.1) (4.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (1.18.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth==2025.1.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth==2025.1.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth==2025.1.1) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth==2025.1.1) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.1.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.1.1) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.4.0->unsloth==2025.1.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth==2025.1.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth==2025.1.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth==2025.1.1) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.1.1) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth==2025.1.1) (1.17.0)\n",
      "Downloading unsloth-2025.1.1-py3-none-any.whl (177 kB)\n",
      "Downloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.14.0-py3-none-any.whl (313 kB)\n",
      "Downloading unsloth_zoo-2025.1.5-py3-none-any.whl (80 kB)\n",
      "Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.13-py3-none-any.whl (115 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m176.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, shtab, safetensors, regex, hf_transfer, fsspec, dill, multiprocess, tyro, tokenizers, transformers, cut_cross_entropy, bitsandbytes, accelerate, peft, datasets, trl, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.12.0 requires fsspec==2024.12.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.3.0 bitsandbytes-0.45.1 cut_cross_entropy-25.1.1 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 hf_transfer-0.1.9 multiprocess-0.70.16 peft-0.14.0 regex-2024.11.6 safetensors-0.5.2 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.21.0 transformers-4.48.2 trl-0.14.0 tyro-0.9.13 unsloth-2025.1.1 unsloth_zoo-2025.1.5 xxhash-3.5.0\n",
      "Found existing installation: transformers 4.48.2\n",
      "Uninstalling transformers-4.48.2:\n",
      "  Successfully uninstalled transformers-4.48.2\n",
      "Collecting transformers==4.47.1\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.1) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.1) (2024.12.14)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.47.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install 'unsloth==2025.1.1'\n",
    "!pip uninstall transformers -y\n",
    "!pip install 'transformers==4.47.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dzn7kugLY7h",
   "metadata": {
    "id": "2dzn7kugLY7h"
   },
   "source": [
    "이 셀에서는 필요한 파이썬 라이브러리를 임포트하고, 미세 조정에 필요한 설정을 정의합니다. `unsloth` 라이브러리에서 `FastLanguageModel` 클래스를 임포트하여 모델 로딩 및 조작을 용이하게 합니다. `torch`는 PyTorch 라이브러리의 핵심 모듈이며, 텐서 연산 및 GPU 가속을 위해 사용됩니다. `max_seq_length` 변수는 모델이 처리할 수 있는 최대 시퀀스 길이를 설정하며, 여기서는 8192 토큰으로 설정되었습니다. `dtype` 변수는 데이터 타입을 정의하며, 여기서는 `None`으로 설정되어 있어 자동으로 감지되도록 합니다. `load_in_4bit` 변수는 모델 가중치를 4비트 정밀도로 로드할지 여부를 결정하며, 여기서는 `True`로 설정되어 메모리 사용량을 줄이고 모델 로딩 속도를 향상시킵니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **`FastLanguageModel` 클래스:** `unsloth` 라이브러리의 핵심 클래스로, 모델 로딩 및 조작을 간편하게 합니다.\n",
    "*   **`torch` 라이브러리:** 텐서 연산, GPU 가속 등 딥러닝 모델링에 필수적인 기능을 제공합니다.\n",
    "*   **최대 시퀀스 길이(`max_seq_length`):** 모델이 처리할 수 있는 최대 입력 토큰 수를 정의합니다. 모델이 긴 문맥을 처리할 수 있도록 설정합니다.\n",
    "*   **데이터 타입(`dtype`):** 모델 가중치 및 연산에 사용될 데이터 타입을 설정합니다. `None`으로 설정하여 자동 감지를 활성화합니다.\n",
    "*   **4비트 로딩(`load_in_4bit`):** 모델 가중치를 4비트 정밀도로 로드하여 메모리 사용량을 줄이고 모델 로딩 시간을 단축합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fBL1gevISCMp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBL1gevISCMp",
    "outputId": "6ca05bfb-0f3d-4409-b02a-da9215d8ffe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Import the FastLanguageModel class from the unsloth library.\n",
    "from unsloth import FastLanguageModel\n",
    "# Import the torch library.\n",
    "import torch\n",
    "\n",
    "# Set the maximum sequence length to 8192 tokens.\n",
    "max_seq_length = 8192\n",
    "\n",
    "# Set the data type to None for automatic detection.\n",
    "dtype = None\n",
    "\n",
    "# Set the load_in_4bit flag to True to load the model weights in 4-bit precision.\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O3X6LNrFLinf",
   "metadata": {
    "id": "O3X6LNrFLinf"
   },
   "source": [
    "이 셀에서는 `unsloth` 라이브러리를 사용하여 Gemma-7b 모델을 로드합니다. `FastLanguageModel.from_pretrained()` 함수를 사용하여 모델과 토크나이저를 로드합니다. `model_name` 매개변수는 로드할 모델의 이름(여기서는 \"unsloth/gemma-7b-bnb-4bit\")을 지정합니다. 이 모델은 4비트 정밀도로 양자화된 버전으로, 메모리 효율적으로 모델을 사용할 수 있도록 해줍니다. 앞서 설정한 `max_seq_length`, `dtype`, `load_in_4bit` 매개변수를 전달하여 모델 로딩을 사용자 정의합니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **`from_pretrained()` 함수:** `unsloth` 라이브러리에서 제공하는 함수로, 사전 학습된 모델과 토크나이저를 편리하게 로드합니다.\n",
    "*   **Gemma-7b 모델:** Google에서 개발한 대규모 언어 모델 중 하나이며, 70억 개의 파라미터를 가지고 있습니다.\n",
    "*   **4비트 양자화 모델:** 모델 가중치를 4비트 정밀도로 양자화하여 메모리 사용량을 줄이고 모델 로딩 시간을 단축합니다. 이는 제한된 GPU 메모리 환경에서 대규모 모델을 효과적으로 사용할 수 있도록 해줍니다.\n",
    "*   **미세 조정 설정:** 미리 정의된 설정(최대 시퀀스 길이, 데이터 타입, 4비트 로딩)을 사용하여 모델을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "UT4MQaVPSVqn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UT4MQaVPSVqn",
    "outputId": "2c6f25c0-08fb-4d57-c90f-02f2ee30d891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.1: Fast Gemma patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 21.951 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c1c7f19ec24a8399450e49b2280fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.57G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0db7ec637244f49686bf051986af8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ffd8ae2897491198d71f5e4dc8382d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9a298745204a40b84a47bc59b4e190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974b941081b34518a684d82462e9e271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0390cbdbdc89418d98cf579efbcb1b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained model from the 'unsloth/gemma-7b-bnb-4bit' repository.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    #model_name = \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    model_name = \"google/gemma-7b\"\n",
    "    \n",
    "    # Set the maximum sequence length to the value defined earlier.\n",
    "    max_seq_length = max_seq_length,\n",
    "\n",
    "    # Set the data type to the value defined earlier.\n",
    "    dtype = dtype,\n",
    "\n",
    "    # Set the load_in_4bit flag to the value defined earlier.\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NO3kOrC4LpRn",
   "metadata": {
    "id": "NO3kOrC4LpRn"
   },
   "source": [
    "이 셀에서는 로드된 모델에 LoRA(Low-Rank Adaptation)를 적용하여 미세 조정 가능한 파라미터 수를 줄입니다. `FastLanguageModel.get_peft_model()` 함수를 사용하여 모델을 래핑하고 LoRA 설정을 적용합니다. `r` 매개변수는 LoRA의 랭크를 설정하며, 16으로 설정되었습니다. `target_modules` 매개변수는 LoRA를 적용할 레이어의 이름을 지정합니다. 여기서는 Transformer 레이어의 핵심 연산 레이어(q_proj, k_proj, v_proj, o_proj 및 MLP 레이어)에 적용하도록 설정되어 있습니다. `lora_alpha` 및 `lora_dropout` 매개변수는 LoRA의 학습률과 드롭아웃 비율을 조절합니다. `bias` 매개변수는 바이어스를 어떻게 처리할지 설정하며, `none`으로 설정하여 바이어스를 사용하지 않습니다. 마지막으로 `use_gradient_checkpointing`를 True로 설정하여 메모리 사용을 최적화합니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **LoRA(Low-Rank Adaptation):** 모델 전체를 업데이트하는 대신 저랭크 행렬을 학습하여 미세 조정 가능한 파라미터 수를 줄이는 기법입니다. 이는 대규모 모델의 미세 조정에 필요한 계산량과 메모리 사용량을 줄여줍니다.\n",
    "*   **LoRA 랭크(`r`):** LoRA 행렬의 랭크를 설정합니다. 낮은 랭크를 사용할수록 모델의 파라미터 수는 줄어들지만, 모델의 표현력에 영향을 줄 수 있습니다. 여기서는 16으로 설정하여 적절한 균형을 유지합니다.\n",
    "*   **타겟 모듈(`target_modules`):** LoRA를 적용할 특정 레이어(Transformer 레이어의 핵심 연산 레이어)를 지정합니다.\n",
    "*   **기타 파라미터:** `lora_alpha`, `lora_dropout`, `bias` 등의 파라미터를 통해 LoRA 학습 과정을 조절합니다.\n",
    "*   **Gradient Checkpointing:** 메모리 사용량을 줄이면서 학습을 수행하는 기술입니다. 큰 모델을 학습할 때 매우 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ta6hKJkSeQw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ta6hKJkSeQw",
    "outputId": "4a2b804f-c8a0-4bf0-8679-36bddf8595ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Create a PEFT model with the given parameters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16, # LoRa Rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3VdQk2iLuun",
   "metadata": {
    "id": "j3VdQk2iLuun"
   },
   "source": [
    "이 셀에서는 학습 데이터를 특정 형식으로 변환하는 함수 `formatted_train`를 정의합니다. 이 함수는 입력 데이터의 구조를 확인하고, 'instruction', 'input', 'output' 필드를 사용하여 텍스트를 특정 형식으로 결합합니다. `eos` 토큰을 추가하여 각 응답이 끝나는 부분을 명확하게 표시합니다. 입력 데이터에 'input' 필드가 있으면, 해당 내용을 포함한 형식으로 만들고, 없으면 'input' 필드 없이 'instruction'과 'output'만 사용해서 텍스트를 만듭니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **학습 데이터 형식:** 학습 데이터를 모델에 입력하기 전에 특정 형식으로 포맷팅하여 모델이 더 효과적으로 학습할 수 있도록 준비합니다.\n",
    "*   **`instruction`, `input`, `output`:** 데이터셋의 각 항목은 'instruction'(지시), 'input'(입력), 'output'(출력) 세 가지 필드를 포함합니다. 이들을 사용하여 모델 학습에 적합한 텍스트를 생성합니다.\n",
    "*   **`eos` 토큰:** 문장의 끝을 나타내는 특수 토큰을 추가하여 모델이 응답의 끝을 명확하게 인식할 수 있도록 합니다. 이는 모델이 불필요하게 응답을 생성하는 것을 방지합니다.\n",
    "*   **조건부 형식:** 입력 데이터에 'input' 필드가 있는지 여부에 따라 다른 형식으로 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "kLm3Ado0Ssk-",
   "metadata": {
    "id": "kLm3Ado0Ssk-"
   },
   "outputs": [],
   "source": [
    "def formatted_train(x):\n",
    "\n",
    "  if x['input']:\n",
    "    formatted_text = f\"\"\"Below is an instruction that describes a task. \\\n",
    "    Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {x['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {x['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {x['output']}<eos>\"\"\"\n",
    "\n",
    "  else:\n",
    "    formatted_text = f\"\"\"Below is an instruction that describes a task. \\\n",
    "    Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {x['instruction']}\n",
    "\n",
    "    ### Response:\n",
    "    {x['output']}<eos>\"\"\"\n",
    "\n",
    "  return formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0pQvqY9LzCT",
   "metadata": {
    "id": "a0pQvqY9LzCT"
   },
   "source": [
    "이 셀에서는 Hugging Face `datasets` 라이브러리를 사용하여 학습 데이터를 로드하고 전처리합니다. `load_dataset()` 함수를 사용하여 지정된 데이터셋(\"TokenBender/code_instructions_122k_alpaca_style\")을 로드합니다. 로드된 데이터셋을 pandas DataFrame으로 변환하고, 앞서 정의한 `formatted_train` 함수를 사용하여 각 데이터 항목을 포맷팅합니다. 마지막으로, 포맷팅된 데이터를 다시 `datasets.Dataset` 객체로 변환합니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **Hugging Face `datasets` 라이브러리:** 대규모 데이터셋을 쉽게 로드하고 관리할 수 있는 라이브러리입니다.\n",
    "*   **데이터셋 로드:** `load_dataset()` 함수를 사용하여 학습 데이터셋을 로드합니다.\n",
    "*   **DataFrame 변환:** 데이터셋을 pandas DataFrame으로 변환하여 데이터 조작을 용이하게 합니다.\n",
    "*   **데이터 포맷팅:** `formatted_train` 함수를 사용하여 데이터셋의 각 항목을 특정 형식으로 변환합니다.\n",
    "*   **Dataset 객체 변환:** 포맷팅된 데이터를 다시 `datasets.Dataset` 객체로 변환하여 모델 학습에 사용할 수 있도록 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "WqlyZD-uSylG",
   "metadata": {
    "id": "WqlyZD-uSylG"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e3f650e56a459fb6b8e1d69d0ef44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561cd2c892c24c22ba6cd0c4197a2051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_instructions_120k.json:   0%|          | 0.00/169M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fe1e05c76e432497fc7aa963ae34ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/121959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def prepare_train_data(data_id):\n",
    "    data = load_dataset(data_id, split=\"train\")\n",
    "    data_df = data.to_pandas()\n",
    "    data_df[\"formatted_text\"] = data_df[[\"input\", \"output\",\n",
    "    \"instruction\"]].apply(formatted_train, axis=1)\n",
    "    data = Dataset.from_pandas(data_df)\n",
    "    return data\n",
    "\n",
    "data_id = \"TokenBender/code_instructions_122k_alpaca_style\"\n",
    "\n",
    "data = prepare_train_data(data_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gx5KBGoML5B7",
   "metadata": {
    "id": "Gx5KBGoML5B7"
   },
   "source": [
    "이 셀에서는 `trl` 라이브러리의 `SFTTrainer` 클래스를 사용하여 모델을 미세 조정합니다. `SFTTrainer`는 Supervised Fine-Tuning(지도 미세 조정)을 위한 트레이너입니다. 모델, 토크나이저, 학습 데이터셋, 데이터셋의 텍스트 필드, 최대 시퀀스 길이 등의 매개변수를 설정합니다. `TrainingArguments` 객체를 사용하여 학습률, 배치 크기, 학습 스텝, 옵티마이저, 로깅 빈도 등 학습과 관련된 하이퍼파라미터를 설정합니다. 특히, `fp16`와 `bf16`을 사용하여 학습 시에 메모리 사용량을 줄이고 학습 속도를 향상시킵니다. `paged_adamw_8bit` 옵티마이저는 메모리 사용량을 줄이는 동시에 학습 성능을 높이는 데 사용됩니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **`SFTTrainer` 클래스:** Supervised Fine-Tuning을 위한 트레이너 클래스입니다. 모델을 사용자 정의된 데이터셋으로 학습시키는 데 필요한 모든 기능을 제공합니다.\n",
    "*   **학습 데이터셋 설정:** 모델 학습에 사용될 데이터셋을 설정합니다.\n",
    "*   **최대 시퀀스 길이:** 모델이 처리할 수 있는 최대 시퀀스 길이를 설정합니다.\n",
    "*   **`TrainingArguments` 객체:** 학습에 필요한 하이퍼파라미터를 설정합니다. 학습률, 배치 크기, 학습 스텝 등을 설정하여 학습 프로세스를 제어합니다.\n",
    "*   **`fp16` 및 `bf16` 혼합 정밀도 학습:** 학습 속도를 향상시키고 메모리 사용량을 줄입니다.\n",
    "*   **`paged_adamw_8bit` 옵티마이저:** 메모리 효율적인 옵티마이저를 사용하여 학습 중에 메모리 문제를 줄입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1oI6AIQ_S1-e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "07ec850285ad474baaf28efba1c2edee",
      "be9cde836b184a6b9a1c7c048ea58444",
      "cbef900921ed4841a15722b02125d4b5",
      "7dc6a5856ad84442823e8cd0fee516a0",
      "5d23597e9eed4e62b36ce3517f5ee638",
      "068b8c6768af4ee081d1548de757171a",
      "52aa65f105dd49b89ebf9e3079595ce5",
      "d6dfc7977a3d4cf49f81d70d363ca1be",
      "e43a32b5c43b43a29722fbf388ff851e",
      "3248263c151b41d291ef12b31806de0b",
      "bac1bf9772ef4e26ae9584641b4a2515"
     ]
    },
    "id": "1oI6AIQ_S1-e",
    "outputId": "bbaca0e0-21bf-4e77-b22a-ad67e8332e2e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450e94e38af147fabd76f932b161ba60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/121959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = data,\n",
    "    dataset_text_field = \"formatted_text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 50,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AU86aHU9L-Jp",
   "metadata": {
    "id": "AU86aHU9L-Jp"
   },
   "source": [
    "이 셀에서는 `trainer.train()` 함수를 호출하여 모델 미세 조정을 시작합니다. `trainer.train()` 함수는 지정된 학습 데이터셋과 하이퍼파라미터를 사용하여 모델을 학습시키고, 학습 과정 동안의 손실과 같은 지표들을 로깅합니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **미세 조정 시작:** `trainer.train()` 함수를 호출하여 모델 미세 조정을 시작합니다.\n",
    "*   **학습 과정 로깅:** 학습 과정 동안의 손실 및 기타 지표들을 로깅하여 학습 진행 상황을 모니터링합니다.\n",
    "*   **Weights & Biases 연동:** 학습 과정의 시각화를 위해 Weights & Biases와 연동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Dpu2RIz4S7Ml",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Dpu2RIz4S7Ml",
    "outputId": "53916d94-ae3d-4b85-88b8-bbf3baa0dffd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 121,959 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 50\n",
      " \"-____-\"     Number of trainable parameters = 50,003,968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 03:19, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.944800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.594200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.719500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.546700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.548500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.541100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.445500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.515700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.523200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TnvFaH3UMDG-",
   "metadata": {
    "id": "TnvFaH3UMDG-"
   },
   "source": [
    "이 셀에서는 테스트 데이터를 생성하기 위해 `format_test` 함수를 정의합니다. 이 함수는 입력 데이터에 'input' 필드가 있는지 확인하고, 'instruction'과 'input'(있는 경우) 필드를 사용하여 테스트 프롬프트를 구성합니다. 학습과 마찬가지로, 입력 데이터에 'input'이 있는 경우와 없는 경우에 따라 서로 다른 형식으로 텍스트를 구성합니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **테스트 프롬프트 형식:** 테스트 프롬프트를 특정 형식으로 생성합니다.\n",
    "*   **`instruction` 및 `input`:** 테스트 프롬프트는 'instruction'과 'input' 필드를 사용합니다.\n",
    "*   **조건부 형식:** 입력 데이터에 'input' 필드가 있는지 여부에 따라 다른 형식으로 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qes7EAf-THBt",
   "metadata": {
    "id": "qes7EAf-THBt"
   },
   "outputs": [],
   "source": [
    "def format_test(x):\n",
    "\n",
    "  if x['input']:\n",
    "    formatted_text = f\"\"\"Below is an instruction that describes a task. \\\n",
    "    Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {x['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {x['input']}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "  else:\n",
    "    formatted_text = f\"\"\"Below is an instruction that describes a task. \\\n",
    "    Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {x['instruction']}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "  return formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QtQs8oNsMGyP",
   "metadata": {
    "id": "QtQs8oNsMGyP"
   },
   "source": [
    "이 셀에서는 학습된 모델을 사용하여 추론을 수행합니다. `format_test` 함수를 사용하여 테스트 프롬프트를 생성하고, `FastLanguageModel.for_inference()`를 사용하여 모델을 추론 모드로 전환합니다. 추론에 최적화된 커널을 활성화하여 추론 속도를 높입니다. 그런 다음, `tokenizer`를 사용하여 프롬프트를 토큰화하고, 생성된 토큰을 GPU로 이동시킵니다. `TextStreamer`를 사용하여 모델이 생성하는 텍스트를 실시간으로 스트리밍합니다. 마지막으로, `model.generate()` 함수를 사용하여 텍스트를 생성하고, `max_new_tokens`를 설정하여 생성되는 최대 토큰 수를 제한합니다.\n",
    "\n",
    "**주요 포인트:**\n",
    "\n",
    "*   **테스트 프롬프트 생성:** `format_test` 함수를 사용하여 테스트 프롬프트를 생성합니다.\n",
    "*   **추론 모드 활성화:** `FastLanguageModel.for_inference()` 함수를 사용하여 모델을 추론 모드로 전환하고 추론 속도를 높입니다.\n",
    "*   **프롬프트 토큰화:** `tokenizer`를 사용하여 테스트 프롬프트를 토큰화하고, 텐서 형태로 변환합니다.\n",
    "*   **GPU 가속:** 토큰화된 텐서를 GPU로 이동시켜 추론 속도를 향상시킵니다.\n",
    "*   **텍스트 스트리밍:** `TextStreamer`를 사용하여 모델이 생성하는 텍스트를 실시간으로 스트리밍합니다.\n",
    "*   **텍스트 생성:** `model.generate()` 함수를 사용하여 텍스트를 생성하고, `max_new_tokens`를 사용하여 생성되는 최대 토큰 수를 제한합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "JdKKlPE7USLO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdKKlPE7USLO",
    "outputId": "67fbc12d-29c0-4688-a966-fea001796a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.     Write a response that appropriately completes the request.\n",
      "\n",
      "    ### Instruction:\n",
      "    Create a way to encrypt a message using a key in Python.\n",
      "\n",
      "    ### Input:\n",
      "    message = \"Hello world!\"\n",
      "key = \"secret\"\n",
      "\n",
      "    ### Response:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "Prompt = format_test(data[155])\n",
    "print(Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qKF-PPKvUUUd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKF-PPKvUUUd",
    "outputId": "91ae0746-578c-4ec4-a94f-55a870e6c920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Below is an instruction that describes a task.     Write a response that appropriately completes the request.\n",
      "\n",
      "    ### Instruction:\n",
      "    Create a way to encrypt a message using a key in Python.\n",
      "\n",
      "    ### Input:\n",
      "    message = \"Hello world!\"\n",
      "key = \"secret\"\n",
      "\n",
      "    ### Response:\n",
      "def encrypt(message, key):\n",
      "    encrypted_message = \"\"\n",
      "    for i in range(len(message)):\n",
      "        if message[i] == \" \":\n",
      "            encrypted_message += \" \"\n",
      "        else:\n",
      "            encrypted_message += chr(ord(message[i]) + ord(key[i % len(key)]))\n",
      "    return encrypted_message\n",
      "\n",
      "print(encrypt(message, key))<eos>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    Prompt\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o_4uvWAaUmdL",
   "metadata": {
    "id": "o_4uvWAaUmdL"
   },
   "source": [
    "오 모델이 정상적으로, 동작하는 것을 확인하였습니다.\n",
    "\n",
    "Finetuned Model을 나중에라도 쓸수있게 저장하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4Hg8rgj7UbIs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Hg8rgj7UbIs",
    "outputId": "9ca6008e-9bea-4639-cf1b-6fd864d6b4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved locally to fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the local directory where you want to save the model.\n",
    "local_model_path = \"fine_tuned_model\"\n",
    "\n",
    "# 2. Save the model and tokenizer locally\n",
    "trainer.save_model(local_model_path)\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "print(f\"Model and tokenizer saved locally to {local_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z3cJgiCjYtFh",
   "metadata": {
    "id": "z3cJgiCjYtFh"
   },
   "source": [
    "해당하는 모델을 GCS Bucket에 UPloading하게습니다.\n",
    "\n",
    "사전에 Bucket이 있다면 해당 Bucket을 사용하고 없다면 아래에 있는 command를 이용해서 만들어 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PrVArnsRY5k3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrVArnsRY5k3",
    "outputId": "d2dc393a-c70c-451e-be5b-10a1a3c08eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://vertexai-unsloth-yourname/...\n",
      "Creating gs://vertexai-unsloth-yourname/...\n"
     ]
    }
   ],
   "source": [
    "# prompt: make gcs bucket with given bucket name in gcloud command\n",
    "\n",
    "BUCKET_NAME=\"vertexai-unsloth-yourname\"\n",
    "gcs_model_path = \"fine_tuned_gemma_model\" # The directory inside the bucket where the model will be uploaded\n",
    "\n",
    "# uncomment the below line if you want to make a bucket.\n",
    "# !gsutil rb gs://$BUCKET_NAME\n",
    "# !gsutil mb -l asia-northeast3 gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C-EAUZg9YkGb",
   "metadata": {
    "id": "C-EAUZg9YkGb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "\n",
    "# 4. Authenticate with Google Cloud Storage\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NrYX6-I7ZUiR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrYX6-I7ZUiR",
    "outputId": "d01755ed-64df-4bbd-e274-1037184d0df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded fine_tuned_model/adapter_config.json to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/adapter_config.json\n",
      "Uploaded fine_tuned_model/training_args.bin to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/training_args.bin\n",
      "Uploaded fine_tuned_model/special_tokens_map.json to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/special_tokens_map.json\n",
      "Uploaded fine_tuned_model/adapter_model.safetensors to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/adapter_model.safetensors\n",
      "Uploaded fine_tuned_model/README.md to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/README.md\n",
      "Uploaded fine_tuned_model/tokenizer_config.json to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/tokenizer_config.json\n",
      "Uploaded fine_tuned_model/tokenizer.model to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/tokenizer.model\n",
      "Uploaded fine_tuned_model/tokenizer.json to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/tokenizer.json\n",
      "\n",
      "Model successfully uploaded to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model\n"
     ]
    }
   ],
   "source": [
    "def upload_directory_to_gcs(local_path, bucket, gcs_path):\n",
    "    \"\"\"Uploads all files in a local directory to GCS, preserving the directory structure.\"\"\"\n",
    "    for root, _, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            local_file = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_file, local_path)\n",
    "            gcs_file_path = os.path.join(gcs_path, relative_path).replace(\"\\\\\", \"/\")  # For windows compatibility\n",
    "            blob = bucket.blob(gcs_file_path)\n",
    "            blob.upload_from_filename(local_file)\n",
    "            print(f\"Uploaded {local_file} to gs://{bucket.name}/{gcs_file_path}\")\n",
    "\n",
    "upload_directory_to_gcs(local_model_path, bucket, gcs_model_path)\n",
    "\n",
    "print(f\"\\nModel successfully uploaded to gs://{BUCKET_NAME}/{gcs_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ef273-37b0-48f6-b994-2868efe0e40a",
   "metadata": {},
   "source": [
    "## Fine Tuned Model 생성 완료\n",
    "\n",
    "로컬에 Fine Tuned Model을 저장하여고, GCS에 해당 모델을 업로드 하였다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brDv2HqHmEFM",
   "metadata": {
    "id": "brDv2HqHmEFM"
   },
   "source": [
    "## Vertex AI에 Fine Tuned Model Deploy\n",
    "\n",
    "기존에 있던 모델활용은 해당 Workspace(또는 Google Colab)환경에서 실제 모델을 읽고 Local에서 활용하기 위한 방법입니다.\n",
    "\n",
    "타인 또는 외부용 서비스로 제공하기 위해서는 공식적인 Endpoint를 구성하여 Scalibility와 Resiliency를 보장해야 합니다.\n",
    "\n",
    "이를 위하여, 현재 구성된 모델을 Model Repository에 등록합니다.\n",
    "\n",
    "이후, 등록된 모델을 Vertex AI Endpoint에 배포하고, 이를 활요하여 Inference를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_P-JNAHmaBHI",
   "metadata": {
    "id": "_P-JNAHmaBHI"
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part, ChatSession\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from google.cloud import aiplatform\n",
    "# from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "znCi27-AWDBc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "znCi27-AWDBc",
    "outputId": "d0236625-dc47-4014-c226-f229354786c9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'turnkey-charter-358922'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: get PROJECT_ID using gcloud command and store this value into PROJECT_ID variable in python\n",
    "\n",
    "PROJECT_ID = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6UBO2hGKnSnS",
   "metadata": {
    "id": "6UBO2hGKnSnS"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"vertexai-unsloth-yourname\"\n",
    "gcs_model_path = \"fine_tuned_gemma_model\" # The directory inside the bucket where the model will be uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PXS6up-vVlj_",
   "metadata": {
    "id": "PXS6up-vVlj_"
   },
   "outputs": [],
   "source": [
    "\n",
    "REGION = \"asia-northeast3\" # Replace with your region\n",
    "GCS_MODEL_PATH = f\"gs://{BUCKET_NAME}/{gcs_model_path}\" # Replace with your GCS path\n",
    "ENDPOINT_NAME = \"fine-tuned-gemma-endpoint\" # Replace with your endpoint name\n",
    "DEPLOYED_MODEL_NAME = \"fine-tuned-gemma-model\" # Replace with your model name\n",
    "MODEL_DISPLAY_NAME = \"fine-tuned-gemma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rpw4di7JV2JF",
   "metadata": {
    "id": "Rpw4di7JV2JF"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xQfXav7TWluR",
   "metadata": {
    "id": "xQfXav7TWluR"
   },
   "outputs": [],
   "source": [
    "def deploy_model_to_vertex_ai():\n",
    "\n",
    "    model_upload = aiplatform.Model.upload(\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        artifact_uri=GCS_MODEL_PATH,\n",
    "        description=\"Fine-tuned Gemma model\",\n",
    "        is_regionalized=True,\n",
    "    )\n",
    "\n",
    "    # Model Deploy\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=ENDPOINT_NAME,\n",
    "        description=\"Endpoint for fine-tuned Gemma model\",\n",
    "    )\n",
    "\n",
    "    deployed_model = endpoint.deploy(\n",
    "        model=model_upload,\n",
    "        deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
    "        machine_type=\"g2-standard-4\",  # g2 인스턴스 사용 (L4 GPU 포함)\n",
    "        accelerator_type=\"NVIDIA_L4\",  # L4 GPU 사용 명시\n",
    "        accelerator_count=1, # GPU 개수 설정\n",
    "    )\n",
    "\n",
    "    print(f\"Model deployed to endpoint: {endpoint.resource_name}\")\n",
    "    print(f\"Deployed Model Resource Name: {deployed_model.resource_name}\")\n",
    "\n",
    "    return deployed_model.id, endpoint.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ycfDrNGxW5g1",
   "metadata": {
    "id": "ycfDrNGxW5g1"
   },
   "outputs": [],
   "source": [
    "def predict_with_vertex_ai(deployed_model_id, endpoint_id, prompt):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a deployed Vertex AI model and returns the response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Vertex AI prediction client\n",
    "    model = aiplatform.DeployedModel(\n",
    "        endpoint=endpoint_id,\n",
    "        deployed_model=deployed_model_id,\n",
    "    )\n",
    "\n",
    "    # Prepare Prediction input parameter\n",
    "    instances = [\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "        }\n",
    "    ]\n",
    "    parameters = {\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_output_tokens\": 512,\n",
    "    }\n",
    "\n",
    "\n",
    "    # Get predictions\n",
    "    prediction = model.predict(instances=instances, parameters=parameters)\n",
    "\n",
    "    # Extract response\n",
    "    response = prediction.predictions[0][\"content\"]\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1VAS1viJW6Ss",
   "metadata": {
    "id": "1VAS1viJW6Ss"
   },
   "outputs": [],
   "source": [
    "# 배포 및 모델 ID 가져오기\n",
    "deployed_model_id, endpoint_id = deploy_model_to_vertex_ai()\n",
    "\n",
    "# 추론할 프롬프트 설정\n",
    "test_prompt = \"\"\"Below is an instruction that describes a task.\n",
    "Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Create a way to encrypt a message using a key in Python.\n",
    "### Input:\n",
    "message = \"Hello world!\"\n",
    "key = \"secret\"\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# 추론 실행\n",
    "response = predict_with_vertex_ai(deployed_model_id, endpoint_id, test_prompt)\n",
    "print(\"\\n----- Prediction Result -----\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "L4_Workshop",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "068b8c6768af4ee081d1548de757171a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07ec850285ad474baaf28efba1c2edee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be9cde836b184a6b9a1c7c048ea58444",
       "IPY_MODEL_cbef900921ed4841a15722b02125d4b5",
       "IPY_MODEL_7dc6a5856ad84442823e8cd0fee516a0"
      ],
      "layout": "IPY_MODEL_5d23597e9eed4e62b36ce3517f5ee638"
     }
    },
    "3248263c151b41d291ef12b31806de0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52aa65f105dd49b89ebf9e3079595ce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d23597e9eed4e62b36ce3517f5ee638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dc6a5856ad84442823e8cd0fee516a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3248263c151b41d291ef12b31806de0b",
      "placeholder": "​",
      "style": "IPY_MODEL_bac1bf9772ef4e26ae9584641b4a2515",
      "value": " 121959/121959 [00:43&lt;00:00, 2553.52 examples/s]"
     }
    },
    "bac1bf9772ef4e26ae9584641b4a2515": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be9cde836b184a6b9a1c7c048ea58444": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_068b8c6768af4ee081d1548de757171a",
      "placeholder": "​",
      "style": "IPY_MODEL_52aa65f105dd49b89ebf9e3079595ce5",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "cbef900921ed4841a15722b02125d4b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6dfc7977a3d4cf49f81d70d363ca1be",
      "max": 121959,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e43a32b5c43b43a29722fbf388ff851e",
      "value": 121959
     }
    },
    "d6dfc7977a3d4cf49f81d70d363ca1be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e43a32b5c43b43a29722fbf388ff851e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07933467-0daf-4bcc-bb58-2c49012cb053",
   "metadata": {},
   "source": [
    "# Unslothë¥¼ ì´ìš©í•œ Gemma Model Fine Tuning\n",
    "\n",
    "Unslothë¥¼ ì´ìš©í•˜ë©´ ë©”ëª¨ë¦¬ Foot Printë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ, PEFTë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "íŒ¨í‚¤ì§€ ì„¤ì¹˜, ë°ì´í„°ì…‹ êµ¬ì„±, íŠ¸ë ˆì´ë‹ ì´í›„ í…ŒìŠ¤íŠ¸ê¹Œì§€ ì§„í–‰í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uD-9Dvs-OYx-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD-9Dvs-OYx-",
    "outputId": "1aa936f9-7c57-4c0a-ab9b-0de6d5d30bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WRbM14IOLVn8",
   "metadata": {
    "id": "WRbM14IOLVn8"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” í•„ìš”í•œ Python íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. `torch`, `torchvision`, `torchaudio`, `xformers`ëŠ” PyTorch ìƒíƒœê³„ì˜ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë©°, ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í•™ìŠµí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. íŠ¹íˆ, `xformers`ëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ë¹ ë¥¸ ì—°ì‚°ì„ ìœ„í•œ ìµœì í™”ëœ ì—°ì‚°ìë¥¼ ì œê³µí•©ë‹ˆë‹¤. `unsloth`ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ ë‹¨ìˆœí™”í•˜ê³  ìµœì í™”í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ëª¨ë¸ ë¡œë”©, ë¯¸ì„¸ ì¡°ì •, ì¶”ë¡  ê³¼ì •ì„ ê°„í¸í•˜ê²Œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, `transformers`ëŠ” Hugging Faceì—ì„œ ê°œë°œí•œ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ ì…€ì—ì„œëŠ” `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ íŠ¹ì • ë²„ì „ìœ¼ë¡œ ê°•ì œ ì„¤ì¹˜í•˜ì—¬ í˜¸í™˜ì„± ë¬¸ì œë¥¼ ë°©ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **PyTorch ìƒíƒœê³„:** ë”¥ëŸ¬ë‹ ëª¨ë¸ ê°œë°œì— í•„ìˆ˜ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n",
    "*   **`unsloth` ë¼ì´ë¸ŒëŸ¬ë¦¬:** ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•œ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì„¤ì¹˜í•¨ìœ¼ë¡œì¨ ë¯¸ì„¸ ì¡°ì • ê³¼ì •ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.\n",
    "*   **`transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬:** íŠ¹ì • ë²„ì „ìœ¼ë¡œ ê³ ì •í•˜ì—¬ í˜¸í™˜ì„± ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤. ì´ëŠ” íŠ¹ì • ë²„ì „ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ìµœì í™”ëœ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°„ì˜ ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "*   **GPU ê°€ì†:** CUDA 12.1 ë²„ì „ì— ë§ì¶° PyTorch ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì—¬ GPU ê°€ì†ì„ í™œìš©í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vOcc74fvJrCacslv9BjJSkvi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "vOcc74fvJrCacslv9BjJSkvi",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5c07beee-b8c7-4cd9-88e2-99ff6c2024a1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (780.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m177.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting xformers\n",
      "  Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m165.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m195.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m189.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m178.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.29.post1-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers, torchvision, torchaudio\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0 xformers-0.0.29.post1\n",
      "Collecting unsloth==2025.1.1\n",
      "  Downloading unsloth-2025.1.1-py3-none-any.whl.metadata (60 kB)\n",
      "Collecting unsloth_zoo>=2025.1.1 (from unsloth==2025.1.1)\n",
      "  Downloading unsloth_zoo-2025.1.5-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (2.5.1+cu121)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (0.0.29.post1)\n",
      "Collecting bitsandbytes (from unsloth==2025.1.1)\n",
      "  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: triton>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (3.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (24.2)\n",
      "Collecting tyro (from unsloth==2025.1.1)\n",
      "  Downloading tyro-0.9.13-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting transformers!=4.47.0,>=4.46.1 (from unsloth==2025.1.1)\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets>=2.16.0 (from unsloth==2025.1.1)\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth==2025.1.1)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (5.9.3)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (0.45.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (1.26.4)\n",
      "Collecting accelerate>=0.34.1 (from unsloth==2025.1.1)\n",
      "  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth==2025.1.1)\n",
      "  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth==2025.1.1)\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from unsloth==2025.1.1) (0.27.1)\n",
      "Collecting hf_transfer (from unsloth==2025.1.1)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.1->unsloth==2025.1.1) (6.0.2)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.1->unsloth==2025.1.1)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (15.0.2)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth==2025.1.1)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (2.32.3)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth==2025.1.1)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth==2025.1.1)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth==2025.1.1)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth==2025.1.1) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->unsloth==2025.1.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.4.0->unsloth==2025.1.1) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.4.0->unsloth==2025.1.1) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth==2025.1.1) (1.3.0)\n",
      "Collecting regex!=2019.12.17 (from transformers!=4.47.0,>=4.46.1->unsloth==2025.1.1)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers!=4.47.0,>=4.46.1->unsloth==2025.1.1)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.1.1) (13.9.4)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.1.1->unsloth==2025.1.1)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from unsloth_zoo>=2025.1.1->unsloth==2025.1.1) (11.0.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth==2025.1.1) (0.16)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth==2025.1.1)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth==2025.1.1) (4.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth==2025.1.1) (1.18.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth==2025.1.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth==2025.1.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth==2025.1.1) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth==2025.1.1) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.1.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.1.1) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.4.0->unsloth==2025.1.1) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth==2025.1.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth==2025.1.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth==2025.1.1) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth==2025.1.1) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth==2025.1.1) (1.17.0)\n",
      "Downloading unsloth-2025.1.1-py3-none-any.whl (177 kB)\n",
      "Downloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.14.0-py3-none-any.whl (313 kB)\n",
      "Downloading unsloth_zoo-2025.1.5-py3-none-any.whl (80 kB)\n",
      "Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.13-py3-none-any.whl (115 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m176.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: sentencepiece, xxhash, shtab, safetensors, regex, hf_transfer, fsspec, dill, multiprocess, tyro, tokenizers, transformers, cut_cross_entropy, bitsandbytes, accelerate, peft, datasets, trl, unsloth_zoo, unsloth\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.12.0 requires fsspec==2024.12.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.3.0 bitsandbytes-0.45.1 cut_cross_entropy-25.1.1 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 hf_transfer-0.1.9 multiprocess-0.70.16 peft-0.14.0 regex-2024.11.6 safetensors-0.5.2 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.21.0 transformers-4.48.2 trl-0.14.0 tyro-0.9.13 unsloth-2025.1.1 unsloth_zoo-2025.1.5 xxhash-3.5.0\n",
      "Found existing installation: transformers 4.48.2\n",
      "Uninstalling transformers-4.48.2:\n",
      "  Successfully uninstalled transformers-4.48.2\n",
      "Collecting transformers==4.47.1\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.47.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.1) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.47.1) (2024.12.14)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "Installing collected packages: transformers\n",
      "Successfully installed transformers-4.47.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install 'unsloth==2025.1.1'\n",
    "!pip uninstall transformers -y\n",
    "!pip install 'transformers==4.47.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dzn7kugLY7h",
   "metadata": {
    "id": "2dzn7kugLY7h"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” í•„ìš”í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ê³ , ë¯¸ì„¸ ì¡°ì •ì— í•„ìš”í•œ ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤. `unsloth` ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ `FastLanguageModel` í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•˜ì—¬ ëª¨ë¸ ë¡œë”© ë° ì¡°ì‘ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. `torch`ëŠ” PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í•µì‹¬ ëª¨ë“ˆì´ë©°, í…ì„œ ì—°ì‚° ë° GPU ê°€ì†ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. `max_seq_length` ë³€ìˆ˜ëŠ” ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•˜ë©°, ì—¬ê¸°ì„œëŠ” 8192 í† í°ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. `dtype` ë³€ìˆ˜ëŠ” ë°ì´í„° íƒ€ì…ì„ ì •ì˜í•˜ë©°, ì—¬ê¸°ì„œëŠ” `None`ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ ìë™ìœ¼ë¡œ ê°ì§€ë˜ë„ë¡ í•©ë‹ˆë‹¤. `load_in_4bit` ë³€ìˆ˜ëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë¡œë“œí• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ë©°, ì—¬ê¸°ì„œëŠ” `True`ë¡œ ì„¤ì •ë˜ì–´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ëª¨ë¸ ë¡œë”© ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **`FastLanguageModel` í´ë˜ìŠ¤:** `unsloth` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í•µì‹¬ í´ë˜ìŠ¤ë¡œ, ëª¨ë¸ ë¡œë”© ë° ì¡°ì‘ì„ ê°„í¸í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "*   **`torch` ë¼ì´ë¸ŒëŸ¬ë¦¬:** í…ì„œ ì—°ì‚°, GPU ê°€ì† ë“± ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ì— í•„ìˆ˜ì ì¸ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "*   **ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´(`max_seq_length`):** ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì…ë ¥ í† í° ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ê¸´ ë¬¸ë§¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "*   **ë°ì´í„° íƒ€ì…(`dtype`):** ëª¨ë¸ ê°€ì¤‘ì¹˜ ë° ì—°ì‚°ì— ì‚¬ìš©ë  ë°ì´í„° íƒ€ì…ì„ ì„¤ì •í•©ë‹ˆë‹¤. `None`ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìë™ ê°ì§€ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
    "*   **4ë¹„íŠ¸ ë¡œë”©(`load_in_4bit`):** ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ëª¨ë¸ ë¡œë”© ì‹œê°„ì„ ë‹¨ì¶•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fBL1gevISCMp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBL1gevISCMp",
    "outputId": "6ca05bfb-0f3d-4409-b02a-da9215d8ffe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Import the FastLanguageModel class from the unsloth library.\n",
    "from unsloth import FastLanguageModel\n",
    "# Import the torch library.\n",
    "import torch\n",
    "\n",
    "# Set the maximum sequence length to 8192 tokens.\n",
    "max_seq_length = 8192\n",
    "\n",
    "# Set the data type to None for automatic detection.\n",
    "dtype = None\n",
    "\n",
    "# Set the load_in_4bit flag to True to load the model weights in 4-bit precision.\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O3X6LNrFLinf",
   "metadata": {
    "id": "O3X6LNrFLinf"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” `unsloth` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Gemma-7b ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. `FastLanguageModel.from_pretrained()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. `model_name` ë§¤ê°œë³€ìˆ˜ëŠ” ë¡œë“œí•  ëª¨ë¸ì˜ ì´ë¦„(ì—¬ê¸°ì„œëŠ” \"unsloth/gemma-7b-bnb-4bit\")ì„ ì§€ì •í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ì–‘ìí™”ëœ ë²„ì „ìœ¼ë¡œ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤. ì•ì„œ ì„¤ì •í•œ `max_seq_length`, `dtype`, `load_in_4bit` ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•˜ì—¬ ëª¨ë¸ ë¡œë”©ì„ ì‚¬ìš©ì ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **`from_pretrained()` í•¨ìˆ˜:** `unsloth` ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” í•¨ìˆ˜ë¡œ, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í¸ë¦¬í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "*   **Gemma-7b ëª¨ë¸:** Googleì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì´ë©°, 70ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "*   **4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸:** ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ì–‘ìí™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  ëª¨ë¸ ë¡œë”© ì‹œê°„ì„ ë‹¨ì¶•í•©ë‹ˆë‹¤. ì´ëŠ” ì œí•œëœ GPU ë©”ëª¨ë¦¬ í™˜ê²½ì—ì„œ ëŒ€ê·œëª¨ ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤.\n",
    "*   **ë¯¸ì„¸ ì¡°ì • ì„¤ì •:** ë¯¸ë¦¬ ì •ì˜ëœ ì„¤ì •(ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´, ë°ì´í„° íƒ€ì…, 4ë¹„íŠ¸ ë¡œë”©)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "UT4MQaVPSVqn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UT4MQaVPSVqn",
    "outputId": "2c6f25c0-08fb-4d57-c90f-02f2ee30d891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.1: Fast Gemma patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 21.951 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c1c7f19ec24a8399450e49b2280fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.57G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0db7ec637244f49686bf051986af8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ffd8ae2897491198d71f5e4dc8382d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9a298745204a40b84a47bc59b4e190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974b941081b34518a684d82462e9e271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0390cbdbdc89418d98cf579efbcb1b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pre-trained model from the 'unsloth/gemma-7b-bnb-4bit' repository.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    #model_name = \"unsloth/gemma-7b-bnb-4bit\",\n",
    "    model_name = \"google/gemma-7b\"\n",
    "    \n",
    "    # Set the maximum sequence length to the value defined earlier.\n",
    "    max_seq_length = max_seq_length,\n",
    "\n",
    "    # Set the data type to the value defined earlier.\n",
    "    dtype = dtype,\n",
    "\n",
    "    # Set the load_in_4bit flag to the value defined earlier.\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NO3kOrC4LpRn",
   "metadata": {
    "id": "NO3kOrC4LpRn"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” ë¡œë“œëœ ëª¨ë¸ì— LoRA(Low-Rank Adaptation)ë¥¼ ì ìš©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì…ë‹ˆë‹¤. `FastLanguageModel.get_peft_model()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë˜í•‘í•˜ê³  LoRA ì„¤ì •ì„ ì ìš©í•©ë‹ˆë‹¤. `r` ë§¤ê°œë³€ìˆ˜ëŠ” LoRAì˜ ë­í¬ë¥¼ ì„¤ì •í•˜ë©°, 16ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤. `target_modules` ë§¤ê°œë³€ìˆ˜ëŠ” LoRAë¥¼ ì ìš©í•  ë ˆì´ì–´ì˜ ì´ë¦„ì„ ì§€ì •í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” Transformer ë ˆì´ì–´ì˜ í•µì‹¬ ì—°ì‚° ë ˆì´ì–´(q_proj, k_proj, v_proj, o_proj ë° MLP ë ˆì´ì–´)ì— ì ìš©í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. `lora_alpha` ë° `lora_dropout` ë§¤ê°œë³€ìˆ˜ëŠ” LoRAì˜ í•™ìŠµë¥ ê³¼ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. `bias` ë§¤ê°œë³€ìˆ˜ëŠ” ë°”ì´ì–´ìŠ¤ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ ì„¤ì •í•˜ë©°, `none`ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë°”ì´ì–´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ `use_gradient_checkpointing`ë¥¼ Trueë¡œ ì„¤ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **LoRA(Low-Rank Adaptation):** ëª¨ë¸ ì „ì²´ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ëŒ€ì‹  ì €ë­í¬ í–‰ë ¬ì„ í•™ìŠµí•˜ì—¬ ë¯¸ì„¸ ì¡°ì • ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. ì´ëŠ” ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ë¯¸ì„¸ ì¡°ì •ì— í•„ìš”í•œ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤.\n",
    "*   **LoRA ë­í¬(`r`):** LoRA í–‰ë ¬ì˜ ë­í¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ë‚®ì€ ë­í¬ë¥¼ ì‚¬ìš©í• ìˆ˜ë¡ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” ì¤„ì–´ë“¤ì§€ë§Œ, ëª¨ë¸ì˜ í‘œí˜„ë ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 16ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì ì ˆí•œ ê· í˜•ì„ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "*   **íƒ€ê²Ÿ ëª¨ë“ˆ(`target_modules`):** LoRAë¥¼ ì ìš©í•  íŠ¹ì • ë ˆì´ì–´(Transformer ë ˆì´ì–´ì˜ í•µì‹¬ ì—°ì‚° ë ˆì´ì–´)ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "*   **ê¸°íƒ€ íŒŒë¼ë¯¸í„°:** `lora_alpha`, `lora_dropout`, `bias` ë“±ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ LoRA í•™ìŠµ ê³¼ì •ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.\n",
    "*   **Gradient Checkpointing:** ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. í° ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ta6hKJkSeQw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ta6hKJkSeQw",
    "outputId": "4a2b804f-c8a0-4bf0-8679-36bddf8595ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Create a PEFT model with the given parameters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16, # LoRa Rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3VdQk2iLuun",
   "metadata": {
    "id": "j3VdQk2iLuun"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” í•™ìŠµ ë°ì´í„°ë¥¼ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ `formatted_train`ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê³ , 'instruction', 'input', 'output' í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ê²°í•©í•©ë‹ˆë‹¤. `eos` í† í°ì„ ì¶”ê°€í•˜ì—¬ ê° ì‘ë‹µì´ ëë‚˜ëŠ” ë¶€ë¶„ì„ ëª…í™•í•˜ê²Œ í‘œì‹œí•©ë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°ì— 'input' í•„ë“œê°€ ìˆìœ¼ë©´, í•´ë‹¹ ë‚´ìš©ì„ í¬í•¨í•œ í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ê³ , ì—†ìœ¼ë©´ 'input' í•„ë“œ ì—†ì´ 'instruction'ê³¼ 'output'ë§Œ ì‚¬ìš©í•´ì„œ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **í•™ìŠµ ë°ì´í„° í˜•ì‹:** í•™ìŠµ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ì „ì— íŠ¹ì • í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ì—¬ ëª¨ë¸ì´ ë” íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
    "*   **`instruction`, `input`, `output`:** ë°ì´í„°ì…‹ì˜ ê° í•­ëª©ì€ 'instruction'(ì§€ì‹œ), 'input'(ì…ë ¥), 'output'(ì¶œë ¥) ì„¸ ê°€ì§€ í•„ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì´ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "*   **`eos` í† í°:** ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ìˆ˜ í† í°ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì´ ì‘ë‹µì˜ ëì„ ëª…í™•í•˜ê²Œ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë¶ˆí•„ìš”í•˜ê²Œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "*   **ì¡°ê±´ë¶€ í˜•ì‹:** ì…ë ¥ ë°ì´í„°ì— 'input' í•„ë“œê°€ ìˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "kLm3Ado0Ssk-",
   "metadata": {
    "id": "kLm3Ado0Ssk-"
   },
   "outputs": [],
   "source": [
    "def formatted_train(x):\n",
    "\n",
    "  if x['input']:\n",
    "    formatted_text = f\"\"\"Below is an instruction that describes a task. \\\n",
    "    Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {x['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {x['input']}\n",
    "\n",
    "    ### Response:\n",
    "    {x['output']}<eos>\"\"\"\n",
    "\n",
    "  else:\n",
    "    formatted_text = f\"\"\"Below is an instruction that describes a task. \\\n",
    "    Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {x['instruction']}\n",
    "\n",
    "    ### Response:\n",
    "    {x['output']}<eos>\"\"\"\n",
    "\n",
    "  return formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0pQvqY9LzCT",
   "metadata": {
    "id": "a0pQvqY9LzCT"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” Hugging Face `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. `load_dataset()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ ë°ì´í„°ì…‹(\"TokenBender/code_instructions_122k_alpaca_style\")ì„ ë¡œë“œí•©ë‹ˆë‹¤. ë¡œë“œëœ ë°ì´í„°ì…‹ì„ pandas DataFrameìœ¼ë¡œ ë³€í™˜í•˜ê³ , ì•ì„œ ì •ì˜í•œ `formatted_train` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë°ì´í„° í•­ëª©ì„ í¬ë§·íŒ…í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, í¬ë§·íŒ…ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ `datasets.Dataset` ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **Hugging Face `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬:** ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ë¡œë“œí•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "*   **ë°ì´í„°ì…‹ ë¡œë“œ:** `load_dataset()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "*   **DataFrame ë³€í™˜:** ë°ì´í„°ì…‹ì„ pandas DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„° ì¡°ì‘ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "*   **ë°ì´í„° í¬ë§·íŒ…:** `formatted_train` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì˜ ê° í•­ëª©ì„ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "*   **Dataset ê°ì²´ ë³€í™˜:** í¬ë§·íŒ…ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ `datasets.Dataset` ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "WqlyZD-uSylG",
   "metadata": {
    "id": "WqlyZD-uSylG"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e3f650e56a459fb6b8e1d69d0ef44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561cd2c892c24c22ba6cd0c4197a2051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_instructions_120k.json:   0%|          | 0.00/169M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fe1e05c76e432497fc7aa963ae34ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/121959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def prepare_train_data(data_id):\n",
    "    data = load_dataset(data_id, split=\"train\")\n",
    "    data_df = data.to_pandas()\n",
    "    data_df[\"formatted_text\"] = data_df[[\"input\", \"output\",\n",
    "    \"instruction\"]].apply(formatted_train, axis=1)\n",
    "    data = Dataset.from_pandas(data_df)\n",
    "    return data\n",
    "\n",
    "data_id = \"TokenBender/code_instructions_122k_alpaca_style\"\n",
    "\n",
    "data = prepare_train_data(data_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gx5KBGoML5B7",
   "metadata": {
    "id": "Gx5KBGoML5B7"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” `trl` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `SFTTrainer` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤. `SFTTrainer`ëŠ” Supervised Fine-Tuning(ì§€ë„ ë¯¸ì„¸ ì¡°ì •)ì„ ìœ„í•œ íŠ¸ë ˆì´ë„ˆì…ë‹ˆë‹¤. ëª¨ë¸, í† í¬ë‚˜ì´ì €, í•™ìŠµ ë°ì´í„°ì…‹, ë°ì´í„°ì…‹ì˜ í…ìŠ¤íŠ¸ í•„ë“œ, ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ë“±ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. `TrainingArguments` ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, í•™ìŠµ ìŠ¤í…, ì˜µí‹°ë§ˆì´ì €, ë¡œê¹… ë¹ˆë„ ë“± í•™ìŠµê³¼ ê´€ë ¨ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. íŠ¹íˆ, `fp16`ì™€ `bf16`ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì‹œì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³  í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤. `paged_adamw_8bit` ì˜µí‹°ë§ˆì´ì €ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ë™ì‹œì— í•™ìŠµ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **`SFTTrainer` í´ë˜ìŠ¤:** Supervised Fine-Tuningì„ ìœ„í•œ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ëª¨ë¸ì„ ì‚¬ìš©ì ì •ì˜ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "*   **í•™ìŠµ ë°ì´í„°ì…‹ ì„¤ì •:** ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë  ë°ì´í„°ì…‹ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "*   **ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´:** ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "*   **`TrainingArguments` ê°ì²´:** í•™ìŠµì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, í•™ìŠµ ìŠ¤í… ë“±ì„ ì„¤ì •í•˜ì—¬ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œì–´í•©ë‹ˆë‹¤.\n",
    "*   **`fp16` ë° `bf16` í˜¼í•© ì •ë°€ë„ í•™ìŠµ:** í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.\n",
    "*   **`paged_adamw_8bit` ì˜µí‹°ë§ˆì´ì €:** ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì¤‘ì— ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ ì¤„ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1oI6AIQ_S1-e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "07ec850285ad474baaf28efba1c2edee",
      "be9cde836b184a6b9a1c7c048ea58444",
      "cbef900921ed4841a15722b02125d4b5",
      "7dc6a5856ad84442823e8cd0fee516a0",
      "5d23597e9eed4e62b36ce3517f5ee638",
      "068b8c6768af4ee081d1548de757171a",
      "52aa65f105dd49b89ebf9e3079595ce5",
      "d6dfc7977a3d4cf49f81d70d363ca1be",
      "e43a32b5c43b43a29722fbf388ff851e",
      "3248263c151b41d291ef12b31806de0b",
      "bac1bf9772ef4e26ae9584641b4a2515"
     ]
    },
    "id": "1oI6AIQ_S1-e",
    "outputId": "bbaca0e0-21bf-4e77-b22a-ad67e8332e2e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450e94e38af147fabd76f932b161ba60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/121959 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = data,\n",
    "    dataset_text_field = \"formatted_text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 50,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AU86aHU9L-Jp",
   "metadata": {
    "id": "AU86aHU9L-Jp"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” `trainer.train()` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤. `trainer.train()` í•¨ìˆ˜ëŠ” ì§€ì •ëœ í•™ìŠµ ë°ì´í„°ì…‹ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , í•™ìŠµ ê³¼ì • ë™ì•ˆì˜ ì†ì‹¤ê³¼ ê°™ì€ ì§€í‘œë“¤ì„ ë¡œê¹…í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **ë¯¸ì„¸ ì¡°ì • ì‹œì‘:** `trainer.train()` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "*   **í•™ìŠµ ê³¼ì • ë¡œê¹…:** í•™ìŠµ ê³¼ì • ë™ì•ˆì˜ ì†ì‹¤ ë° ê¸°íƒ€ ì§€í‘œë“¤ì„ ë¡œê¹…í•˜ì—¬ í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.\n",
    "*   **Weights & Biases ì—°ë™:** í•™ìŠµ ê³¼ì •ì˜ ì‹œê°í™”ë¥¼ ìœ„í•´ Weights & Biasesì™€ ì—°ë™í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Dpu2RIz4S7Ml",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Dpu2RIz4S7Ml",
    "outputId": "53916d94-ae3d-4b85-88b8-bbf3baa0dffd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 121,959 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 50\n",
      " \"-____-\"     Number of trainable parameters = 50,003,968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 03:19, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.944800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.594200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.719500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.557200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.546700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.548500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.541100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.445500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.515700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.639500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.486200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.523200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TnvFaH3UMDG-",
   "metadata": {
    "id": "TnvFaH3UMDG-"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ `format_test` í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ ë°ì´í„°ì— 'input' í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , 'instruction'ê³¼ 'input'(ìˆëŠ” ê²½ìš°) í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. í•™ìŠµê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ì…ë ¥ ë°ì´í„°ì— 'input'ì´ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš°ì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ í˜•ì‹:** í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "*   **`instruction` ë° `input`:** í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ëŠ” 'instruction'ê³¼ 'input' í•„ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "*   **ì¡°ê±´ë¶€ í˜•ì‹:** ì…ë ¥ ë°ì´í„°ì— 'input' í•„ë“œê°€ ìˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qes7EAf-THBt",
   "metadata": {
    "id": "qes7EAf-THBt"
   },
   "outputs": [],
   "source": [
    "def format_test(x):\n",
    "\n",
    "  if x['input']:\n",
    "    formatted_text = f\"\"\"Below is an instruction that describes a task. \\\n",
    "    Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {x['instruction']}\n",
    "\n",
    "    ### Input:\n",
    "    {x['input']}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "  else:\n",
    "    formatted_text = f\"\"\"Below is an instruction that describes a task. \\\n",
    "    Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {x['instruction']}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "\n",
    "  return formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QtQs8oNsMGyP",
   "metadata": {
    "id": "QtQs8oNsMGyP"
   },
   "source": [
    "ì´ ì…€ì—ì„œëŠ” í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. `format_test` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê³ , `FastLanguageModel.for_inference()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤. ì¶”ë¡ ì— ìµœì í™”ëœ ì»¤ë„ì„ í™œì„±í™”í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ, `tokenizer`ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•˜ê³ , ìƒì„±ëœ í† í°ì„ GPUë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤. `TextStreamer`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, `model.generate()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , `max_new_tokens`ë¥¼ ì„¤ì •í•˜ì—¬ ìƒì„±ë˜ëŠ” ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” í¬ì¸íŠ¸:**\n",
    "\n",
    "*   **í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±:** `format_test` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "*   **ì¶”ë¡  ëª¨ë“œ í™œì„±í™”:** `FastLanguageModel.for_inference()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜í•˜ê³  ì¶”ë¡  ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.\n",
    "*   **í”„ë¡¬í”„íŠ¸ í† í°í™”:** `tokenizer`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•˜ê³ , í…ì„œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "*   **GPU ê°€ì†:** í† í°í™”ëœ í…ì„œë¥¼ GPUë¡œ ì´ë™ì‹œì¼œ ì¶”ë¡  ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n",
    "*   **í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°:** `TextStreamer`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.\n",
    "*   **í…ìŠ¤íŠ¸ ìƒì„±:** `model.generate()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , `max_new_tokens`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë˜ëŠ” ìµœëŒ€ í† í° ìˆ˜ë¥¼ ì œí•œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "JdKKlPE7USLO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdKKlPE7USLO",
    "outputId": "67fbc12d-29c0-4688-a966-fea001796a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.     Write a response that appropriately completes the request.\n",
      "\n",
      "    ### Instruction:\n",
      "    Create a way to encrypt a message using a key in Python.\n",
      "\n",
      "    ### Input:\n",
      "    message = \"Hello world!\"\n",
      "key = \"secret\"\n",
      "\n",
      "    ### Response:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "Prompt = format_test(data[155])\n",
    "print(Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qKF-PPKvUUUd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKF-PPKvUUUd",
    "outputId": "91ae0746-578c-4ec4-a94f-55a870e6c920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Below is an instruction that describes a task.     Write a response that appropriately completes the request.\n",
      "\n",
      "    ### Instruction:\n",
      "    Create a way to encrypt a message using a key in Python.\n",
      "\n",
      "    ### Input:\n",
      "    message = \"Hello world!\"\n",
      "key = \"secret\"\n",
      "\n",
      "    ### Response:\n",
      "def encrypt(message, key):\n",
      "    encrypted_message = \"\"\n",
      "    for i in range(len(message)):\n",
      "        if message[i] == \" \":\n",
      "            encrypted_message += \" \"\n",
      "        else:\n",
      "            encrypted_message += chr(ord(message[i]) + ord(key[i % len(key)]))\n",
      "    return encrypted_message\n",
      "\n",
      "print(encrypt(message, key))<eos>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    Prompt\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o_4uvWAaUmdL",
   "metadata": {
    "id": "o_4uvWAaUmdL"
   },
   "source": [
    "ì˜¤ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ, ë™ì‘í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "Finetuned Modelì„ ë‚˜ì¤‘ì—ë¼ë„ ì“¸ìˆ˜ìˆê²Œ ì €ì¥í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4Hg8rgj7UbIs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Hg8rgj7UbIs",
    "outputId": "9ca6008e-9bea-4639-cf1b-6fd864d6b4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved locally to fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the local directory where you want to save the model.\n",
    "local_model_path = \"fine_tuned_model\"\n",
    "\n",
    "# 2. Save the model and tokenizer locally\n",
    "trainer.save_model(local_model_path)\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "print(f\"Model and tokenizer saved locally to {local_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z3cJgiCjYtFh",
   "metadata": {
    "id": "z3cJgiCjYtFh"
   },
   "source": [
    "í•´ë‹¹í•˜ëŠ” ëª¨ë¸ì„ GCS Bucketì— UPloadingí•˜ê²ŒìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ì „ì— Bucketì´ ìˆë‹¤ë©´ í•´ë‹¹ Bucketì„ ì‚¬ìš©í•˜ê³  ì—†ë‹¤ë©´ ì•„ë˜ì— ìˆëŠ” commandë¥¼ ì´ìš©í•´ì„œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PrVArnsRY5k3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrVArnsRY5k3",
    "outputId": "d2dc393a-c70c-451e-be5b-10a1a3c08eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://vertexai-unsloth-yourname/...\n",
      "Creating gs://vertexai-unsloth-yourname/...\n"
     ]
    }
   ],
   "source": [
    "# prompt: make gcs bucket with given bucket name in gcloud command\n",
    "\n",
    "BUCKET_NAME=\"vertexai-unsloth-yourname\"\n",
    "gcs_model_path = \"fine_tuned_gemma_model\" # The directory inside the bucket where the model will be uploaded\n",
    "\n",
    "# uncomment the below line if you want to make a bucket.\n",
    "# !gsutil rb gs://$BUCKET_NAME\n",
    "# !gsutil mb -l asia-northeast3 gs://$BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C-EAUZg9YkGb",
   "metadata": {
    "id": "C-EAUZg9YkGb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "\n",
    "# 4. Authenticate with Google Cloud Storage\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NrYX6-I7ZUiR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrYX6-I7ZUiR",
    "outputId": "d01755ed-64df-4bbd-e274-1037184d0df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded fine_tuned_model/adapter_config.json to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/adapter_config.json\n",
      "Uploaded fine_tuned_model/training_args.bin to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/training_args.bin\n",
      "Uploaded fine_tuned_model/special_tokens_map.json to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/special_tokens_map.json\n",
      "Uploaded fine_tuned_model/adapter_model.safetensors to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/adapter_model.safetensors\n",
      "Uploaded fine_tuned_model/README.md to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/README.md\n",
      "Uploaded fine_tuned_model/tokenizer_config.json to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/tokenizer_config.json\n",
      "Uploaded fine_tuned_model/tokenizer.model to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/tokenizer.model\n",
      "Uploaded fine_tuned_model/tokenizer.json to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model/tokenizer.json\n",
      "\n",
      "Model successfully uploaded to gs://vertexai-unsloth-yourname/fine_tuned_gemma_model\n"
     ]
    }
   ],
   "source": [
    "def upload_directory_to_gcs(local_path, bucket, gcs_path):\n",
    "    \"\"\"Uploads all files in a local directory to GCS, preserving the directory structure.\"\"\"\n",
    "    for root, _, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            local_file = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_file, local_path)\n",
    "            gcs_file_path = os.path.join(gcs_path, relative_path).replace(\"\\\\\", \"/\")  # For windows compatibility\n",
    "            blob = bucket.blob(gcs_file_path)\n",
    "            blob.upload_from_filename(local_file)\n",
    "            print(f\"Uploaded {local_file} to gs://{bucket.name}/{gcs_file_path}\")\n",
    "\n",
    "upload_directory_to_gcs(local_model_path, bucket, gcs_model_path)\n",
    "\n",
    "print(f\"\\nModel successfully uploaded to gs://{BUCKET_NAME}/{gcs_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ef273-37b0-48f6-b994-2868efe0e40a",
   "metadata": {},
   "source": [
    "## Fine Tuned Model ìƒì„± ì™„ë£Œ\n",
    "\n",
    "ë¡œì»¬ì— Fine Tuned Modelì„ ì €ì¥í•˜ì—¬ê³ , GCSì— í•´ë‹¹ ëª¨ë¸ì„ ì—…ë¡œë“œ í•˜ì˜€ë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brDv2HqHmEFM",
   "metadata": {
    "id": "brDv2HqHmEFM"
   },
   "source": [
    "## Vertex AIì— Fine Tuned Model Deploy\n",
    "\n",
    "ê¸°ì¡´ì— ìˆë˜ ëª¨ë¸í™œìš©ì€ í•´ë‹¹ Workspace(ë˜ëŠ” Google Colab)í™˜ê²½ì—ì„œ ì‹¤ì œ ëª¨ë¸ì„ ì½ê³  Localì—ì„œ í™œìš©í•˜ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤.\n",
    "\n",
    "íƒ€ì¸ ë˜ëŠ” ì™¸ë¶€ìš© ì„œë¹„ìŠ¤ë¡œ ì œê³µí•˜ê¸° ìœ„í•´ì„œëŠ” ê³µì‹ì ì¸ Endpointë¥¼ êµ¬ì„±í•˜ì—¬ Scalibilityì™€ Resiliencyë¥¼ ë³´ì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ë¥¼ ìœ„í•˜ì—¬, í˜„ì¬ êµ¬ì„±ëœ ëª¨ë¸ì„ Model Repositoryì— ë“±ë¡í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´í›„, ë“±ë¡ëœ ëª¨ë¸ì„ Vertex AI Endpointì— ë°°í¬í•˜ê³ , ì´ë¥¼ í™œìš”í•˜ì—¬ Inferenceë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_P-JNAHmaBHI",
   "metadata": {
    "id": "_P-JNAHmaBHI"
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part, ChatSession\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from google.cloud import aiplatform\n",
    "# from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "znCi27-AWDBc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "znCi27-AWDBc",
    "outputId": "d0236625-dc47-4014-c226-f229354786c9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'turnkey-charter-358922'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: get PROJECT_ID using gcloud command and store this value into PROJECT_ID variable in python\n",
    "\n",
    "PROJECT_ID = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6UBO2hGKnSnS",
   "metadata": {
    "id": "6UBO2hGKnSnS"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME=\"vertexai-unsloth-yourname\"\n",
    "gcs_model_path = \"fine_tuned_gemma_model\" # The directory inside the bucket where the model will be uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PXS6up-vVlj_",
   "metadata": {
    "id": "PXS6up-vVlj_"
   },
   "outputs": [],
   "source": [
    "\n",
    "REGION = \"asia-northeast3\" # Replace with your region\n",
    "GCS_MODEL_PATH = f\"gs://{BUCKET_NAME}/{gcs_model_path}\" # Replace with your GCS path\n",
    "ENDPOINT_NAME = \"fine-tuned-gemma-endpoint\" # Replace with your endpoint name\n",
    "DEPLOYED_MODEL_NAME = \"fine-tuned-gemma-model\" # Replace with your model name\n",
    "MODEL_DISPLAY_NAME = \"fine-tuned-gemma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rpw4di7JV2JF",
   "metadata": {
    "id": "Rpw4di7JV2JF"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xQfXav7TWluR",
   "metadata": {
    "id": "xQfXav7TWluR"
   },
   "outputs": [],
   "source": [
    "def deploy_model_to_vertex_ai():\n",
    "\n",
    "    model_upload = aiplatform.Model.upload(\n",
    "        display_name=MODEL_DISPLAY_NAME,\n",
    "        artifact_uri=GCS_MODEL_PATH,\n",
    "        description=\"Fine-tuned Gemma model\",\n",
    "        is_regionalized=True,\n",
    "    )\n",
    "\n",
    "    # Model Deploy\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=ENDPOINT_NAME,\n",
    "        description=\"Endpoint for fine-tuned Gemma model\",\n",
    "    )\n",
    "\n",
    "    deployed_model = endpoint.deploy(\n",
    "        model=model_upload,\n",
    "        deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
    "        machine_type=\"g2-standard-4\",  # g2 ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© (L4 GPU í¬í•¨)\n",
    "        accelerator_type=\"NVIDIA_L4\",  # L4 GPU ì‚¬ìš© ëª…ì‹œ\n",
    "        accelerator_count=1, # GPU ê°œìˆ˜ ì„¤ì •\n",
    "    )\n",
    "\n",
    "    print(f\"Model deployed to endpoint: {endpoint.resource_name}\")\n",
    "    print(f\"Deployed Model Resource Name: {deployed_model.resource_name}\")\n",
    "\n",
    "    return deployed_model.id, endpoint.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ycfDrNGxW5g1",
   "metadata": {
    "id": "ycfDrNGxW5g1"
   },
   "outputs": [],
   "source": [
    "def predict_with_vertex_ai(deployed_model_id, endpoint_id, prompt):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a deployed Vertex AI model and returns the response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Vertex AI prediction client\n",
    "    model = aiplatform.DeployedModel(\n",
    "        endpoint=endpoint_id,\n",
    "        deployed_model=deployed_model_id,\n",
    "    )\n",
    "\n",
    "    # Prepare Prediction input parameter\n",
    "    instances = [\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "        }\n",
    "    ]\n",
    "    parameters = {\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_output_tokens\": 512,\n",
    "    }\n",
    "\n",
    "\n",
    "    # Get predictions\n",
    "    prediction = model.predict(instances=instances, parameters=parameters)\n",
    "\n",
    "    # Extract response\n",
    "    response = prediction.predictions[0][\"content\"]\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1VAS1viJW6Ss",
   "metadata": {
    "id": "1VAS1viJW6Ss"
   },
   "outputs": [],
   "source": [
    "# ë°°í¬ ë° ëª¨ë¸ ID ê°€ì ¸ì˜¤ê¸°\n",
    "deployed_model_id, endpoint_id = deploy_model_to_vertex_ai()\n",
    "\n",
    "# ì¶”ë¡ í•  í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "test_prompt = \"\"\"Below is an instruction that describes a task.\n",
    "Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Create a way to encrypt a message using a key in Python.\n",
    "### Input:\n",
    "message = \"Hello world!\"\n",
    "key = \"secret\"\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "# ì¶”ë¡  ì‹¤í–‰\n",
    "response = predict_with_vertex_ai(deployed_model_id, endpoint_id, test_prompt)\n",
    "print(\"\\n----- Prediction Result -----\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "L4_Workshop",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "068b8c6768af4ee081d1548de757171a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07ec850285ad474baaf28efba1c2edee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be9cde836b184a6b9a1c7c048ea58444",
       "IPY_MODEL_cbef900921ed4841a15722b02125d4b5",
       "IPY_MODEL_7dc6a5856ad84442823e8cd0fee516a0"
      ],
      "layout": "IPY_MODEL_5d23597e9eed4e62b36ce3517f5ee638"
     }
    },
    "3248263c151b41d291ef12b31806de0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52aa65f105dd49b89ebf9e3079595ce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d23597e9eed4e62b36ce3517f5ee638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dc6a5856ad84442823e8cd0fee516a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3248263c151b41d291ef12b31806de0b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bac1bf9772ef4e26ae9584641b4a2515",
      "value": "â€‡121959/121959â€‡[00:43&lt;00:00,â€‡2553.52â€‡examples/s]"
     }
    },
    "bac1bf9772ef4e26ae9584641b4a2515": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be9cde836b184a6b9a1c7c048ea58444": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_068b8c6768af4ee081d1548de757171a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_52aa65f105dd49b89ebf9e3079595ce5",
      "value": "Mapâ€‡(num_proc=2):â€‡100%"
     }
    },
    "cbef900921ed4841a15722b02125d4b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6dfc7977a3d4cf49f81d70d363ca1be",
      "max": 121959,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e43a32b5c43b43a29722fbf388ff851e",
      "value": 121959
     }
    },
    "d6dfc7977a3d4cf49f81d70d363ca1be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e43a32b5c43b43a29722fbf388ff851e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
